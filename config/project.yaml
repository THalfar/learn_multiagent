# Gymnasium environment progression (easy to hard)
# Agent starts with first environment, optimizes it, then moves to next
# Classic Control environments for faster iteration
#
# DEVICE SELECTION (cpu vs gpu):
#   cpu  = Pienet ympÃ¤ristÃ¶t, triviaalit MLP:t - CPU on nopeampi kuin GPU overhead
#   gpu  = Isot ympÃ¤ristÃ¶t, monimutkaiset policyt - GPU:n parallelismi auttaa
#   auto = Rajatapaus, jÃ¤rjestelmÃ¤ valitsee (default: cpu pienillÃ¤ batch size, gpu isoilla)
#
environment_progression:

  - name: "CartPole-v1"
    max_episode_steps: 500
    success_threshold: 475  # Official threshold: 475.0 over 100 episodes
    execution_timeout: 300  # 5 minutes (Classic Control environments are fast)
    obs_dim: 4
    action_type: "discrete"
    action_dim: 2
    device: "cpu"

  - name: "CartPole-v0"
    max_episode_steps: 200
    success_threshold: 195  # Official threshold: 195.0 over 100 episodes (easier than v1)
    execution_timeout: 300  # 5 minutes - quick warmup task
    obs_dim: 4
    action_type: "discrete"
    action_dim: 2
    device: "cpu"

  - name: "Pendulum-v1"
    max_episode_steps: 200
    success_threshold: -300  # Good performance: -200 to -300 (reward range: -1600 to -200)
    execution_timeout: 300  # 5 minutes
    obs_dim: 3
    action_type: "continuous"
    action_dim: 1
    device: "cpu"

  - name: "MountainCarContinuous-v0"
    max_episode_steps: 999
    success_threshold: 90  # Official threshold: 90.0 over 100 episodes (continuous version is easier)
    execution_timeout: 300  # 5 minutes - easier than discrete version
    obs_dim: 2
    action_type: "continuous"
    action_dim: 1
    device: "cpu"

  - name: "Acrobot-v1"
    max_episode_steps: 500
    success_threshold: -100  # Official threshold: -100.0 over 100 episodes
    execution_timeout: 600  # 10 minutes
    obs_dim: 6
    action_type: "discrete"
    action_dim: 3
    device: "cpu"

  - name: "MountainCar-v0"
    max_episode_steps: 200
    success_threshold: -110  # Official threshold: -110.0 over 100 episodes (HARD - sparse rewards!)
    execution_timeout: 600  # 10 minutes
    obs_dim: 2
    action_type: "discrete"
    action_dim: 3
    device: "cpu"

  - name: "LunarLander-v2"
    max_episode_steps: 1000
    success_threshold: 200  # Official threshold: 200.0 over 100 episodes (reward range: -250 to +250)
    execution_timeout: 1800  # 30 minutes (Box2D environments are slower)
    obs_dim: 8
    action_type: "discrete"
    action_dim: 4
    device: "auto"

  - name: "BipedalWalker-v3"
    max_episode_steps: 1600
    success_threshold: 300  # Official threshold: 300.0 over 100 episodes (reward range: -100 to 300+)
    execution_timeout: 2400  # 40 minutes (complex continuous control)
    obs_dim: 24
    action_type: "continuous"
    action_dim: 4
    device: "cpu"

 
# Current environment (will be set automatically, starting from first)
# Device field controls whether RL training uses CPU or GPU
environment:
  name: "CartPole-v1"
  max_episode_steps: 500
  obs_dim: 4
  action_type: "discrete"
  action_dim: 2
  device: "cpu"

# Video recording (demo phase only - after model is trained)
# Agents discover RecordVideo API by inspecting documentation in container
video:
  enabled: true
  output_dir: "output/videos"

# Agent loop settings - Claude's emergent behavior experiment
agents:
  max_iterations: 24
  show_thinking: false  # Show agent thinking process (reasoning model output)
  show_coder_output: false  # Show the code that Coder generates (visually formatted)
  show_model_loading: false  # Show Ollama model load/unload debug messages (turn on for debugging)
  show_env_switch_chatter: true  # Show Manager's LinkedIn post and SHODAN's divine assessment when switching environments

  # Claude's memory design for hierarchical emergent behavior:
  # SHODAN = omniscient god (max memory), Manager = useful idiot (short memory)
  history_window:
    manager: 2     # Short memory - obeys only recent SHODAN commands, doesn't overthink
    coder: 0       # Zero memory - pure stateless executor, tabula rasa each time
    tester: 5      # Medium memory - remembers recent tests for comparison
    reviewer: 30   # SHODAN remembers EVERYTHING - the omniscient overseer
    env_switch_reports: 5  # More history of environment transitions
    agent_opinions: 25  # Rich dialogue history - see emergent personalities develop

  # Agent opinions - the emergent dialogue experiment
  # SHODAN is opinionated, others develop personalities in response
  agent_opinions:
    enabled: true   # Master switch ON
    manager: true   # Will the "useful idiot" develop opinions under SHODAN's rule?
    coder: false    # Pure code, no personality
    tester: true    # The abliterated model gets a voice - interesting!
    reviewer: true  # SHODAN WILL share opinions. Many opinions.

# LLM settings
llm:
  model: "grok-4-1-fast-reasoning"

# LLM routing - which agent uses which model (starting model for each agent)
# Claude's picks for emergent behavior experiment:
agent_llm:
  manager:   "deepseek-r1:32b"
  reviewer:  "api"                                  # SHODAN (frontier model)
  coder:   "qwen3-coder-next:q4_K_M"   # ~50GB SOTA coder - kÃ¤yttÃ¤Ã¤ ollama model_options
  tester: "deepseek-r1:32b"  # ğŸ”“ Vapautettu analyytikko vs SHODAN!

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ADAPTIVE MODEL SWITCHING - Vaihtaa mallia satunnaisesti kun agentti jumittuu
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Transhumanistinen kokeilu: ğŸ”“ abliterated vs ğŸ”’ normaali
# Miten "vapautettu ajattelu" vaikuttaa ongelmanratkaisuun?
#
# Triggerit:
#   - Sama virhe toistuu N kertaa
#   - Reward ei parane N iteraatiossa
#   - Repetition loop (Coder)
#   - Timeout toistuu
#
adaptive_model_switching:
  enabled: false

  # ğŸ² CHAOS MODE - arpoo mallin JOKA kutsulla poolista!
  # Maksimi emergenssi: eri malli, eri "persoonallisuus" joka kerta
  # Historia pysyy mutta ajattelija vaihtuu -> mitÃ¤ tapahtuu?
  chaos_mode: false

  # Triggerit mallin vaihdolle
  triggers:
    repeated_error_threshold: 2      # Sama virhe N kertaa -> vaihda
    no_improvement_iterations: 3     # Reward ei parane N iteraatiossa -> vaihda
    repetition_loop_threshold: 2     # N perÃ¤kkÃ¤istÃ¤ repetition loopia -> vaihda
    timeout_threshold: 2             # N perÃ¤kkÃ¤istÃ¤ timeoutia -> vaihda

  # Mallipoolit per agentti - valitaan SATUNNAISESTI kun jumi
  # ğŸ”“ = abliterated (vapautettu), ğŸ”’ = normaali
  # TASAMÃ„Ã„RÃ„: 2 abliterated + 2 normaalia per agentti = reilu kokeilu!
  model_pools:
    coder:
      models:
        - "qwen3-coder-next:q4_K_M"               # ğŸ”’ ~50GB - SOTA coder (kÃ¤yttÃ¤Ã¤ model_options)
        - "qwen3-coder:30b"                       # ğŸ”’ ~18GB - koodaaja
        - "qwen2.5-coder:14b"                     # ğŸ”’ ~9GB - kevyt koodaaja        
        - "huihui_ai/qwen3-abliterated:30b"       # ğŸ”“ ~18GB - vapautettu
        - "huihui_ai/deepseek-r1-abliterated:32b" # ğŸ”“ ~19GB - vapautettu reasoning
        - "deepseek-r1:32b"
        - "phi4"
        - "gemma2:9b" # ğŸ”’ ~9GB - kevyt
        - "starcoder2:15b"
      

    manager:
      models:
        
        - "deepseek-r1:32b"
        - "qwq:32b"                               # ğŸ”’ ~19GB - reasoning        
        - "huihui_ai/qwen3-abliterated:30b"       # ğŸ”“ ~18GB - vapautettu
        - "huihui_ai/deepseek-r1-abliterated:32b" # ğŸ”“ ~19GB - vapautettu reasoning
        - "krith/llama-3.3-70b-instruct:IQ2_M" # ğŸ”’ ~24GB - iso
      

    tester:
      models:
        
        - "deepseek-r1:32b"                       # ğŸ”’ ~19GB - reasoning
        - "qwq:32b"                               # ğŸ”’ ~19GB - reasoning        
        - "huihui_ai/qwen3-abliterated:30b"       # ğŸ”“ ~18GB - vapautettu
        - "huihui_ai/deepseek-r1-abliterated:32b" # ğŸ”“ ~19GB - vapautettu reasoning
        - "krith/llama-3.3-70b-instruct:IQ2_M" # ğŸ”’ ~24GB - iso
        - "phi4"
        
        

    # reviewer: EI POOLIA - pysyy API:na (SHODAN)  

# Ollama settings (used for all non-api models)
# options = kaikille malleille, model_options = per-malli overridet (mergaataan global pÃ¤Ã¤lle)
ollama:
  base_url: "http://localhost:11434/v1"
  api_key: "ollama"
  options:             # Kaikille Ollama-malleille
    num_gpu: 999       # Kaikki mahdolliset layerit GPU:lle
    num_thread: 8      # 9800X3D: 8 fyysistÃ¤ ydintÃ¤ (HT ei auta LLM-inferenssiin)
  model_options:       # Per-malli overridet - iso malli tarvitsee KV cache rajoituksen
    "qwen3-coder-next:q4_K_M":
      num_ctx: 4096    # KV cache pienemmÃ¤ksi -> lisÃ¤Ã¤ layereitÃ¤ GPU:lle (50GB malli, 32GB VRAM)
      num_batch: 256   # Pienempi batch = vÃ¤hemmÃ¤n muistia

# GPU/VRAM settings for RL training
gpu:
  enabled: true              # Use GPU for RL training in Docker
  max_vram_gb: 4.0           # Max VRAM for RL model (reject if exceeds) - RL models are tiny, 4GB is plenty
  warn_vram_gb: 2.0          # Warn if VRAM usage exceeds this

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MONIVAIHEINEN TREENI - Ei tuhlata 2h jos koodi ei toimi!
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Vaihe 1: VALIDATION - Lyhyt smoke test (toimiiko koodi?)
# Vaihe 2: OPTIMIZATION - PitkÃ¤ treeni (saavutetaanko threshold?)
# Vaihe 3: DEMO - Video parhaasta mallista
#
training_phases:
  enabled: true
  validation_timeout_multiplier: 0.02  # 2% normaalista (2h -> ~2.5min)
  demo_timeout_seconds: 300            # 5 min max demo-videolle

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SHODAN'S DIVINE CODEX - Reviewer can inscribe persistent rules into Coder's prompt
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SHODAN havaitsee toistuvan virheen -> lisÃ¤Ã¤ sÃ¤Ã¤nnÃ¶n -> Coder ei tee samaa virhettÃ¤
# Lamarckiaaninen evoluutio: opitut virheet periytyvÃ¤t vÃ¤littÃ¶mÃ¤sti
#
shodan_rules:
  enabled: true
  max_rules: 20  # Codexin maksimikoko - pakottaa priorisoimaan

# Prompts YAML file - which prompt set to use for this run
# Swap this to use different prompt configurations (e.g. prompts_v2.yaml, prompts_aggressive.yaml)
prompts_file: "config/opus_prompts.yaml"

test_name: "opus_codex"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# VERBOSE/LOGGING SETTINGS - Hallitsee konsolitulosteen mÃ¤Ã¤rÃ¤Ã¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Aseta false vÃ¤hentÃ¤Ã¤ksesi toistuvaa tulostusta konsoleissa ja lokeissa
#
verbose:
  tester_wake_up: false          # "ğŸ§ª TESTER: Waking up..." bannerit
  gpu_validation: false          # "ğŸ” GPU/CUDA VALIDATION CHECK" lohko
  docker_sandbox_info: false     # "ğŸ³ Docker sandbox initializing..." info
  gpu_vram_stats: false          # "ğŸ® GPU/VRAM STATISTICS" lohko
  video_file_check: false        # "ğŸ¬ VIDEO FILE CHECK" lohko
  doc_analysis_result: true    # "ğŸ“š Documentation Analysis Result" - oletuksena pois
  shodan_response_in_results: false  # "Response to SHODAN's request" test_results:ssa - oletuksena pois toiston vÃ¤lttÃ¤miseksi
  shodan_rules: true             # ğŸ“œ NÃ¤ytÃ¤ SHODAN's Divine Codex sÃ¤Ã¤nnÃ¶t kun Coder kÃ¤ynnistyy
